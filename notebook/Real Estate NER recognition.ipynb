{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef24759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m567.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, safetensors, tqdm, regex, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.6.0 huggingface-hub-0.15.1 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e110722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 264/264 [00:00<00:00, 462kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.10k/1.10k [00:00<00:00, 2.12MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.59MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 265kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 709M/709M [02:34<00:00, 4.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9997156, 'index': 1, 'word': 'Nad', 'start': 0, 'end': 3}, {'entity': 'I-PER', 'score': 0.9816159, 'index': 2, 'word': '##er', 'start': 3, 'end': 5}, {'entity': 'I-PER', 'score': 0.99977404, 'index': 3, 'word': 'Jo', 'start': 6, 'end': 8}, {'entity': 'I-PER', 'score': 0.9998016, 'index': 4, 'word': '##kha', 'start': 8, 'end': 11}, {'entity': 'I-PER', 'score': 0.9997688, 'index': 5, 'word': '##dar', 'start': 11, 'end': 14}, {'entity': 'B-LOC', 'score': 0.99973196, 'index': 8, 'word': 'Syria', 'start': 25, 'end': 30}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a43b1-7920-4579-9a50-c8edc4fad4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931e4b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.99971443, 'index': 255, 'word': 'София', 'start': 526, 'end': 531}, {'entity': 'B-LOC', 'score': 0.5292786, 'index': 257, 'word': 'кв', 'start': 534, 'end': 536}, {'entity': 'B-LOC', 'score': 0.6424494, 'index': 259, 'word': 'В', 'start': 538, 'end': 539}, {'entity': 'I-LOC', 'score': 0.999178, 'index': 260, 'word': '##ито', 'start': 539, 'end': 542}, {'entity': 'I-LOC', 'score': 0.9995359, 'index': 261, 'word': '##ша', 'start': 542, 'end': 544}, {'entity': 'B-LOC', 'score': 0.71576947, 'index': 279, 'word': 'К', 'start': 589, 'end': 590}, {'entity': 'I-LOC', 'score': 0.9413293, 'index': 280, 'word': '##ау', 'start': 590, 'end': 592}, {'entity': 'I-LOC', 'score': 0.9274997, 'index': 281, 'word': '##ф', 'start': 592, 'end': 593}, {'entity': 'I-LOC', 'score': 0.9522429, 'index': 282, 'word': '##ланд', 'start': 593, 'end': 597}, {'entity': 'B-LOC', 'score': 0.63897717, 'index': 286, 'word': 'Ф', 'start': 602, 'end': 603}, {'entity': 'I-LOC', 'score': 0.7708642, 'index': 287, 'word': '##ант', 'start': 603, 'end': 606}, {'entity': 'I-LOC', 'score': 0.792075, 'index': 288, 'word': '##аст', 'start': 606, 'end': 609}, {'entity': 'I-LOC', 'score': 0.83460754, 'index': 289, 'word': '##ико', 'start': 609, 'end': 612}, {'entity': 'B-ORG', 'score': 0.98129624, 'index': 365, 'word': 'Б', 'start': 814, 'end': 815}, {'entity': 'I-ORG', 'score': 0.7014842, 'index': 366, 'word': '##Д', 'start': 815, 'end': 816}, {'entity': 'I-ORG', 'score': 0.7486037, 'index': 367, 'word': '##С', 'start': 816, 'end': 817}]\n"
     ]
    }
   ],
   "source": [
    "example = \"ВИСОКОКАЧЕСТВЕНО СТРОИТЕЛСТВО * БЕЗ СКОСОВЕ * ПРАВИЛНИ ФОРМИ * ГАЗ * ИЗЦЯЛО ПОКРИТ * ИЗТОК * ВХОДИРАНИ ДОКУМЕНТИ ЗА АКТ16 * ЕКСКЛУЗИВНО ПРЕДЛОЖЕНИЕ САМО В НОВ ДОМ 1 * Детайлна информация на www.novdom1.bg - Реф:74788 Имотът представлява: Дневна, трапезария, кухненски бокс и спален бокс в еднопространствено помещение с обща площ около 30м2 и отделно баня с тоалетна и Тераса около 4м2.;Към имота има прилежащ склад от 23м2. Отлична Локация: Имота се намира в един от най-предпочитаните за живеене и инвестиция райони на град София - кв. Витоша - в непосредствена близост до хипермаркети 'Кауфланд' и 'Фантастико'. Лесен и бърз достъп до главни пътни артерии, административни учреждения, банки, автобусни спирки, детски площадки, аптеки, търговски центрове, супермаркети и др. Състояние: Имотът се издава съгласно БДС Екстериорна блиндирана врата. 12 см. Изолация; 7 камерна дограма с 3-слоен стъклопакет. Под: нивелирана замазка подготвена за полагане на гранитогрес, паркет или настилка по ... Виж повече\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5aa203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 78.0/78.0 [00:00<00:00, 54.9kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 988/988 [00:00<00:00, 2.04MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.79M/1.79M [00:00<00:00, 2.28MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 1.44M/1.44M [00:00<00:00, 5.91MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 452kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 334M/334M [01:09<00:00, 4.81MB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"iarfmoose/roberta-small-bulgarian-ner\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"iarfmoose/roberta-small-bulgarian-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b744d9-e694-40c1-a52c-ecbaaa6fce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "def predict(\n",
    "    text: str, \n",
    "    model: torch.nn.Module, \n",
    "    tokenizer: AutoTokenizer,\n",
    "    labels_tags={\n",
    "        0: \"O\",\n",
    "        1: \"B-PER\", 2: \"I-PER\", \n",
    "        3: \"B-ORG\", 4: \"I-ORG\", \n",
    "        5: \"B-LOC\", 6: \"I-LOC\"\n",
    "    }) -> List[Dict[str, str]]:\n",
    "    tokens_data = tokenizer(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens_data[\"input_ids\"])\n",
    "    words = subwords_to_words(tokens)\n",
    "\n",
    "    input_ids = torch.LongTensor(tokens_data[\"input_ids\"]).unsqueeze(0)\n",
    "    attention_mask = torch.LongTensor(tokens_data[\"attention_mask\"]).unsqueeze(0)\n",
    "\n",
    "    out = model(input_ids, attention_mask=attention_mask).logits\n",
    "    out = out.argmax(-1).squeeze(0).tolist()\n",
    "\n",
    "    prediction = [labels_tags[idx] if idx in labels_tags else idx for idx in out]\n",
    "\n",
    "    return merge_words_and_predictions(words, prediction)\n",
    "\n",
    "\n",
    "def subwords_to_words(tokens: List[str]) -> List[str]:\n",
    "    out_tokens = []\n",
    "    curr_token = \"\"\n",
    "    tags = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == \"[SEP]\":\n",
    "            curr_token = curr_token.replace(\"▁\", \"\")\n",
    "            out_tokens.append(curr_token)\n",
    "            out_tokens.append(\"[SEP]\")\n",
    "            break\n",
    "\n",
    "        if \"▁\" in token and curr_token == \"\":\n",
    "            curr_token += token\n",
    "\n",
    "        elif \"▁\" in token and curr_token != \"\":\n",
    "            curr_token = curr_token.replace(\"▁\", \"\")\n",
    "            out_tokens.append(curr_token)\n",
    "            curr_token = \"\"\n",
    "            curr_token += token\n",
    "\n",
    "        elif \"▁\" not in token:\n",
    "            curr_token += token\n",
    "\n",
    "    return out_tokens\n",
    "\n",
    "\n",
    "def merge_words_and_predictions(words: List[str], entities: List[str]) -> List[Dict[str, str]]:\n",
    "    result = []\n",
    "    curr_word = []\n",
    "\n",
    "    for i, (word, entity) in enumerate(zip(words[1:], entities[1:])):\n",
    "        if \"B-\" in entity:\n",
    "            if curr_word:\n",
    "                curr_word = \" \".join(curr_word)\n",
    "                result.append({\n",
    "                    \"word\": curr_word,\n",
    "                    \"entity_group\": entities[i][2:]\n",
    "                })\n",
    "                curr_word = [word]\n",
    "            else:\n",
    "                curr_word.append(word)\n",
    "\n",
    "        if \"I-\" in entity:\n",
    "            curr_word.append(word)\n",
    "        \n",
    "        if \"O\" == entity:\n",
    "            if curr_word:\n",
    "                curr_word = \" \".join(curr_word)\n",
    "                result.append({\n",
    "                    \"word\": curr_word,\n",
    "                    \"entity_group\": entities[i][2:]\n",
    "                })\n",
    "            \n",
    "            curr_word = []\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a584e6e7-3583-40a3-be1b-725adf95c633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 663/663 [00:00<00:00, 1.33MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 3.34M/3.34M [00:00<00:00, 3.84MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 9.12M/9.12M [00:02<00:00, 4.56MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 41.0/41.0 [00:00<00:00, 68.2kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 173/173 [00:00<00:00, 416kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.83k/1.83k [00:00<00:00, 4.31MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 709M/709M [02:06<00:00, 5.61MB/s] \n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"auhide/bert-bg-ner\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_ID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdae1d58-8c61-40a4-a0ff-4421b704e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ВИСОКОКАЧЕСТВЕНО СТРОИТЕЛСТВО * БЕЗ СКОСОВЕ * ПРАВИЛНИ ФОРМИ * ГАЗ * ИЗЦЯЛО ПОКРИТ * ИЗТОК * ВХОДИРАНИ ДОКУМЕНТИ ЗА АКТ16 * ЕКСКЛУЗИВНО ПРЕДЛОЖЕНИЕ САМО В НОВ ДОМ 1 * Детайлна информация на www.novdom1.bg - Реф:74788 Имотът представлява: Дневна, трапезария, кухненски бокс и спален бокс в еднопространствено помещение с обща площ около 30м2 и отделно баня с тоалетна и Тераса около 4м2.;Към имота има прилежащ склад от 23м2. Отлична Локация: Имота се намира в един от най-предпочитаните за живеене и инвестиция райони на град София - кв. Витоша - в непосредствена близост до хипермаркети 'Кауфланд' и 'Фантастико'. Лесен и бърз достъп до главни пътни артерии, административни учреждения, банки, автобусни спирки, детски площадки, аптеки, търговски центрове, супермаркети и др. Състояние: Имотът се издава съгласно БДС Екстериорна блиндирана врата. 12 см. Изолация; 7 камерна дограма с 3-слоен стъклопакет. Под: нивелирана замазка подготвена за полагане на гранитогрес, паркет или настилка по ... Виж повече\n",
      "NERs: []\n"
     ]
    }
   ],
   "source": [
    "text = \"Барух Спиноза е роден в Амстердам\"\n",
    "print(f\"Input: {example}\")\n",
    "print(\"NERs:\", predict(example, model=model, tokenizer=tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511fcc0-c644-4ba0-a548-b054f0bbc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
